{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31394d'> Practical Exercise: Logistic Regression </font>\n",
    "\n",
    "In this module, we'll be exploring how to build a logistic regression model using scikit-learn. Remember, logistic regression is a binary classification algorithm, so instead of predicting a number (regression) or a group of labels (i.e. multi-class classification) we'll be predicting either true (1) or false (0). Let's start by importing our usual toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#31394d'> Breast Cancer Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). It is a dataset that contains measurements taken on breast cancer cell images. The goal of the dataset is to predict whether a cancer tumor is benign or malignant.\n",
    "\n",
    "\n",
    "![title](media/breast_cancer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer_data = load_breast_cancer()\n",
    "cancer_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a binary classification problem, where the target variable indicates whether a tumor is malignant or benign, encoded as 0 and 1, respectively. The features are measurements taken from the cell images, mostly measures of the cell nucleii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cancer_data[\"target\"]).value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, 62.7% of tumors are benign and 37.3% are malignant.\n",
    "\n",
    "There are lots of features in this dataset. Let's limit ourselves to just the \"mean\" features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cancer_data[\"data\"], columns=cancer_data.feature_names)\n",
    "\n",
    "df = df[df.columns[df.columns.str.startswith('mean')]]\n",
    "\n",
    "df['target'] = cancer_data.target\n",
    "print('The dataset has', df.shape[0], 'rows and', df.shape[1], 'features')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its always a good idea to check that our variables are of the expected data types. Any non-numerical variables would need to be converted to numeric variables (e.g. via one-hot encoding) before we can fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our dataset into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.target, test_size=0.2, random_state=12345)\n",
    "\n",
    "\n",
    "print('The training set has', X_train.shape[0], 'rows')\n",
    "print('The test set has', X_test.shape[0], 'rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortly, we will be fitting our first logistic regression model. From an optimisation point of view, this involves finding the $\\beta$ coefficients that minimise the cross-entropy loss function. The internal procedures that perform this operation work best if the features are *standardized*; that is, for each feature, we subtract its mean and divide by its standard deviation. This ensures that all features have the same scale, but doesn't change the nature of the relationship between the features and the target variable.\n",
    "\n",
    "We can do this in `sklearn` by instantiating a `StandardScaler` object from the `preprocessing` submodule. We use the `fit_transform` method to compute the means and standard deviations for the training data and then apply the transformation to the training data. We then use the `transform` method to standardize the test data using the means and standard deviations estimated from the training set. \n",
    "\n",
    "üôã‚Äç‚ôÄÔ∏è <font color='#eb3483'> Question: </font> Why do we not apply the `fit_transform` method on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train, columns=df.columns[:-1]) # just so you can see what the transformed data looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#31394d'> Logistic Regression in Scikit-Learn </font>\n",
    "\n",
    "In this section, we will fit a logistic regression model and use it to make predictions. Take note of how similar the process is to linear regression. This is the case for all estimators in `sklearn` and is the primary reason for the module's popularity. Even though the models are very different, the training and prediction steps look almost identical in `sklearn`!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#?LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#eb3483'> A side note... </font>\n",
    "\n",
    "Note (from the help file) that `LogisticRegression` has a `penalty` argument that is set to `l2` by default. If you completed the bonus material on linear regression, you will know a bit about *regularization* - a technique that introduces some bias (a simpler model) in an attempt to reduce sampling variability (the bias-variance trade-off!). In linear regression, an `l2` penalty leads to ridge regression, while the `l1` penalty corresponds to the lasso. These penalties can also be applied to the loss function in logistic regression - the only difference is that the loss function is now cross-entropy rather than the residual sum of squares. In the `LogisticRegression` function, the strength of the penalty is controlled by the additional argument `C`, where smaller values correspond to **more** strength (its the inverse of the $\\lambda$ parameter that we discussed in the regularization notebook). The optimal value of `C` can be chosen by cross validation. Importantly, if you are going to use a regularized model, then you **MUST** always standardize your features.\n",
    "\n",
    "We are going to ignore the penalty for the rest of this notebook and fit a standard logistic regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='none') # instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X=X_train, y=y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#31394d'> Interpretation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the estimated coefficients in the usual way. Note that because we standardized our features, the absolute values of the coefficients give us a measure of feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clf.coef_[0], index=df.columns[:-1]).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=df.columns[:-1], y=np.abs(clf.coef_[0]))\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <font color='#eb3483'> Exercise: </font> Area and radius look like the most important features for classifying cancer tumors. How would you explain their coefficients to a medical practitioner who knows nothing about machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(clf.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#31394d'> Predictions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make some predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `predict` method directly outputs the predicted class (0 or 1), assuming a classification threshold of 0.5. If we want to see the underlying probabilities, we can use the `predict_proba` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = clf.predict_proba(X_test)\n",
    "y_prob[:20,:]\n",
    "# why are there two columns of results here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <font color='#eb3483'> Exercise: </font> Using the estimated probabilities, confirm that the `predict` method does indeed use a 0.5 classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#31394d'> Model Evaluation </font>\n",
    "\n",
    "In binary classification, we have: \n",
    "\n",
    "- *Positive cases*: Cases of labelled as 1 (benign cancers)\n",
    "- *Negative cases*: Cases of labelled as 0 (malignant cancers)\n",
    "\n",
    "Since actual positive cases can be classified correctly as positive or incorrectly as negative, and actual negative cases can be classified incorrectly as positive or correctly as negative, we have four possible scenarios:\n",
    "\n",
    "- *True Positives* (TP): the cancers that are benign and are correctly classified as benign\n",
    "- *False Positives* (FP): malignant cancers that are incorrectly classified as benign\n",
    "- *True Negatives* (TN): malignant cancers that are correctly classified as malignant\n",
    "- *False Negatives* (FN): benign cancers that are incorrectly classified as malignant\n",
    "\n",
    "![title](media/classification_errors.png)\n",
    "\n",
    "### <font color='#31394d'> **Confusion Matrix** </font>\n",
    "\n",
    "We can use a confusion matrix to easily examine how a classifier has performed in each one of these categories. The `metrics` submodule in `sklearn` has a `confusion_matrix` function. NB: Read the documentation (`?confusion_matrix`) to see what the rows and columns represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "\n",
    "C = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "pd.DataFrame(C, index=['actual0','actual1'], columns=['pred0','pred1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#31394d'> Evaluation Metrics for Classification </font>\n",
    "\n",
    "#### <font color='#eb3483'> Classification Accuracy </font>\n",
    "\n",
    "Accuracy is a general measure of the model's performance. It simply measures the percentage of cases correctly classified.\n",
    "\n",
    "$$\\text{Accuracy}=\\frac{\\text{Number of correctly classified observations}}{\\text{Total number of observations}}= \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(C[0,0]+C[1,1])/C.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#eb3483'> Precision </font>\n",
    "\n",
    "Precision measures the accuracy of a model's positive predictions - when a model says a case is positive, how confident can we be that it is correct?\n",
    "\n",
    "$$\\text{Precision}=\\frac{\\text{Number of positive cases correctly classified}}{\\text{Number of cases classified as positive}}= \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "<img src=\"media/precision_accuracy.png\" style=\"width:30em;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[1,1]/(C[1,1]+C[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#eb3483'> Recall/ True Positive Rate (TPR) </font>\n",
    " \n",
    "Recall gives us an idea of the model's ability to find (detect) the true positive cases.\n",
    "\n",
    "$$\\text{Recall}=\\frac{\\text{Number of positive cases correctly classified}}{\\text{Number of positive classes}}= \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "\n",
    "![title](media/precision_recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[1,1]/(C[1,1]+C[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#eb3483'> ROC Curve and AUC </font>\n",
    "\n",
    "The receiver operating characteristics [(ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a curve used to evaluate how recall (TPR) and the false positive rate (FPR) vary as we change the classification threshold (that is, the theshold we apply to the probabilities to determine which class a case belongs to). We have alsready defined the TPR above. The FPR is defined as the proportion of true negatives that are incorrected classified as positive:\n",
    "\n",
    "$$\\text{False positive rate} = \\frac{\\text{Number of negative cases incorrectly classified}}{\\text{Total number of negative cases}} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}} $$\n",
    "\n",
    "When the classification threshold is low, the FPR will be high since most cases (negative/positive will be classified as positive). At the same time, TPR will be high since most of the positive cases will be classified as positive. The opposite is true when the classification threshold is high. The ROC curve summarizes this trade off between correctly classifying the positive and negative cases as we vary the classification threshold. \n",
    "\n",
    "Classifiers that perform well have ROC curves that \"hug\" the top left hand corner of the plot. Poor classifiers have ROC curves that are close to diagonal. We can therefore use the area under the curve (AUC) as a classifier evaluation metric: a value close to 1 is good and a value close to 0.5 is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_roc_curve(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, y_prob[:,1]) \n",
    "# NB: Give it the estimated *probabilities*, not the predicted classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> K-Nearest Neighbors for Classification </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <font color='#eb3483'> Exercise: </font> Repeat the above analysis but this time use a $K$-nearest neighbors classifier. Determine which model is best for this problem - logistic regression or a KNN classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
